# -*- coding: utf-8 -*-
"""finaltest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eoCeMuaTtTDrdJIOlDnU9q7teDIDUCDK
"""

#Task 1
#importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer

#Task 2
#dataset loading
dftrain=pd.read_csv('train.csv')
dftest=pd.read_csv('test.csv')
dfsample=pd.read_csv('sample.csv')
print("Train Data:\n",dftrain.head())
print("\nTest Data:\n", dftest.head())
print("\nSample Submission Format:\n", dfsample.head())

#Tak 3 EDA
dftrain.info()

dftrain.shape

#Tak 3 EDA
dftest.info()

dftest.shape

#Task 4 Preprocessing
#droping the loan id column
#traindata
dftrain=dftrain.drop(columns=['Loan_ID'])
dftrain.shape

#test data
dftest=dftest.drop(columns=['Loan_ID'])
dftest.head()

dftrain.isnull().sum()



# Handling missing values in dftrain
# For numerical columns, use median imputation
num_imputer = SimpleImputer(strategy='median')
dftrain[['LoanAmount', 'Loan_Amount_Term', 'ApplicantIncome', 'CoapplicantIncome']] = num_imputer.fit_transform(
    dftrain[['LoanAmount', 'Loan_Amount_Term', 'ApplicantIncome', 'CoapplicantIncome']])

# For categorical columns, use mode imputation
cat_imputer = SimpleImputer(strategy='most_frequent')
dftrain[['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']] = cat_imputer.fit_transform(
    dftrain[['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']])

dftest.isnull().sum()

# Repeat missing value handling for dftest
dftest[['LoanAmount', 'Loan_Amount_Term', 'ApplicantIncome', 'CoapplicantIncome']] = num_imputer.transform(
    dftest[['LoanAmount', 'Loan_Amount_Term', 'ApplicantIncome', 'CoapplicantIncome']])
dftest[['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']] = cat_imputer.transform(
    dftest[['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']])

# Encode categorical features
label_encoder = LabelEncoder()
for col in ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents']:
  #dftrain
    dftrain[col] = label_encoder.fit_transform(dftrain[col])
    #dftest
    dftest[col] = label_encoder.transform(dftest[col])

# Separate features and target in dftrain
X = dftrain.drop(columns=['Loan_Status'])
y = dftrain['Loan_Status'].apply(lambda x: 1 if x == 'Y' else 0)  # Encode target as 1 for 'Y' and 0 for 'N'

scaler = StandardScaler()
X[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']] = scaler.fit_transform(
    X[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']])
dftest[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']] = scaler.transform(
    dftest[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']])

# Display a summary to confirm successful preprocessing
print("Train Data (dftrain):\n", dftrain.head())
print("\nTest Data (dftest):\n", dftest.head())

# Display null values for each column in the training dataset
print(dftrain.isnull().sum())

# Map the target variable in dftrain
dftrain['Loan_Status'] = dftrain['Loan_Status'].map({'Y': 1, 'N': 0})

# Map 'Dependents' column in both dftrain and dftest (if it's ordinal)
dependents_mapping = {'0': 0, '1': 1, '2': 2, '3+': 3}
dftrain['Dependents'] = dftrain['Dependents'].map(dependents_mapping)
dftest['Dependents'] = dftest['Dependents'].map(dependents_mapping)

# Confirm mappings were successful
print("Loan_Status mapping:\n", dftrain['Loan_Status'].unique())
print("Dependents mapping:\n", dftrain['Dependents'].unique())

# Define feature matrix (X) and target variable (y)
X = dftrain.drop(columns=['Loan_Status'])  # Features
y = dftrain['Loan_Status']                 # Target

# Display the shapes to confirm dimensions
print("Feature matrix (X) shape:", X.shape)
print("Target variable (y) shape:", y.shape)

from sklearn.preprocessing import StandardScaler

# Instantiate the scaler
scaler = StandardScaler()

# Identify numerical columns in X that need scaling
numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']

# Fit the scaler on the training data and transform the columns
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Apply the same scaling to the test data (dftest)
dftest[numerical_cols] = scaler.transform(dftest[numerical_cols])

# Display the first few rows to confirm scaling
print("Scaled feature matrix (X):\n", X.head())
print("\nScaled test data (dftest):\n", dftest.head())

# prompt: # Split the data into training and testing sets

# Split the data into training and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# Step 1: Refined parameter grid
param_grid = {
    'n_estimators': [150, 200, 250],  # Focus around the initial best range
    'max_depth': [15, 20, 25],        # Fine-tuning depth
    'min_samples_split': [3, 5, 7],   # Narrower range for minimum samples split
    'min_samples_leaf': [1, 2, 3]     # Test slightly fewer leaf options
}

# Initialize RandomForest model
model = RandomForestClassifier(random_state=42)

# Step 2: Use GridSearchCV with refined parameters
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                      # Cross-validation
    scoring='f1',              # Optimizing for F1 score
    n_jobs=-1,                 # Use all processors
    verbose=1                  # Output progress
)

# Fit the model with grid search on training data
grid_search.fit(X_train, y_train)

# Retrieve the best model and parameters
best_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Step 3: Evaluate on validation set
y_pred = best_model.predict(X_val)
print("\nFine-Tuned Model Performance:")
print("Accuracy:", accuracy_score(y_val, y_pred))
print("Precision:", precision_score(y_val, y_pred))
print("Recall:", recall_score(y_val, y_pred))
print("F1 Score:", f1_score(y_val, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_val, y_pred))

# Step 4: Feature Importance Analysis
importances = best_model.feature_importances_
feature_names = X_train.columns
feature_importance = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)

print("\nFeature Importances:")
for feature, importance in feature_importance:
    print(f"{feature}: {importance:.4f}")

# Evaluate the best model on the validation set
y_pred_val = best_model.predict(X_val)
print("Validation Accuracy:", accuracy_score(y_val, y_pred_val))
print("\nClassification Report:\n", classification_report(y_val, y_pred_val))

# Evaluate the best model on the test set
y_pred_test = best_model.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))
print("\nClassification Report (Test Set):\n", classification_report(y_test, y_pred_test))

# prompt:  Make Predictions on the Test Data

# Make predictions on the test data using the best model
y_pred_test_final = best_model.predict(dftest)

# Create a submission DataFrame
submission = pd.DataFrame({'Loan_ID': dfsample['Loan_ID'], 'Loan_Status': y_pred_test_final})
submission['Loan_Status'] = submission['Loan_Status'].map({1: 'Y', 0: 'N'})

# Display the first few rows of the submission DataFrame
print(submission.head())

# Save the predictions to a CSV file
submission.to_csv('sample_submission.csv', index=False)

import pandas as pd

# Load the sample submission file
sample_submission = pd.read_csv('sample_submission.csv')
print("Sample Submission Format:\n", sample_submission.head())

# prompt: Replace the target column with the prediction values you get using the test
# dataset for your best model.

# Replace the target column in the sample submission file with the predictions
dfsample['Loan_Status'] = y_pred_test_final
dfsample['Loan_Status'] = dfsample['Loan_Status'].map({1: 'Y', 0: 'N'})

# Save the updated sample submission file
dfsample.to_csv('sample_submission.csv', index=False)

# prompt: Save this new dataset as a csv file (search for the code in internet)

# The provided code already includes saving the predictions to a CSV file:
# submission.to_csv('sample_submission.csv', index=False)

# If you want to save a different DataFrame (e.g., dftrain, dftest) as a CSV:
# dftrain.to_csv('dftrain_saved.csv', index=False)
# dftest.to_csv('dftest_saved.csv', index=False)